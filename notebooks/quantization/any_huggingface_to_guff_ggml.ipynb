{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc094eb-85ab-4a88-a9c2-19a3379ad73d",
   "metadata": {},
   "source": [
    "## Evangelista â€“ LLama2 7B Quantizations - GUFF/GGML, AWQ, SqueezeLLM\n",
    "- GGUF. New library that replaces GGML (deprecated). Open source, more extensible and user friendly. [Ref](https://deci.ai/blog/ggml-vs-gguf-comparing-formats-amp-top-5-methods-for-running-gguf)\n",
    "\n",
    "### Build llama.cpp\n",
    "llama.cpp provides the tools to convert model to gguf and quantize it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcbe656-2c83-41e9-bda2-13c08dc01c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt update -y\n",
    "!apt install build-essential cmake -y >/dev/null\n",
    "\n",
    "# Clone llama.cpp\n",
    "!if [ ! -d \"llama.cpp\" ]; then git clone https://github.com/ggerganov/llama.cpp.git; fi\n",
    "%cd llama.cpp\n",
    "\n",
    "# Build llama.cpp\n",
    "!pip install -r requirements.txt\n",
    "!make quantize\n",
    "    \n",
    "%reset -f\n",
    "print('Done!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb9bef-706f-424c-8d08-84dc3ab3bf9e",
   "metadata": {},
   "source": [
    "### Logging to HuggingFace - ONLY needed if your model is gated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7299dc5d-5646-425f-b01c-930df48419ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE You ONLY need to login if your model is gated\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2083c4-8141-45d5-8ec7-54ceaa8ece1c",
   "metadata": {},
   "source": [
    "### Load and Save Your Desired Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83673239-d042-46c9-bc29-42dfbe047826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR MODEL URI BELOW\n",
    "# --------------------------------------------------------------------------------\n",
    "%env HF_MODEL_URI = meta-llama/Llama-2-7b-chat-hf\n",
    "\n",
    "\n",
    "import torch, os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "HF_MODEL_URI = os.environ.get('HF_MODEL_URI')\n",
    "os.environ['MODEL_NAME']=os.path.basename(HF_MODEL_URI)\n",
    "MODEL_NAME = os.environ.get('MODEL_NAME')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    HF_MODEL_URI,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_MODEL_URI,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print('Saving model...')\n",
    "tokenizer.save_pretrained(f\"./models/{MODEL_NAME}\")\n",
    "model.save_pretrained(f'./models/{MODEL_NAME}')\n",
    "print('Done!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b6092-b45b-4082-b7cc-67780ba5dd89",
   "metadata": {},
   "source": [
    "### Convert model to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5baef-a406-430f-9d95-6bb990dca74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./convert.py ./models/$MODEL_NAME --outfile ./models/$MODEL_NAME/$MODEL_NAME.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73700bd6-8d51-4faa-8c33-cf3b2d215567",
   "metadata": {},
   "source": [
    "### Quantize model to Q4_K_M\n",
    "Quantizations Reference: https://github.com/ggerganov/llama.cpp/pull/1684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58ffb6-2919-4b72-a7aa-5feb5a6684d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./quantize ./models/$MODEL_NAME/$MODEL_NAME.gguf ./models/$MODEL_NAME/$MODEL_NAME-Q4_K_M.gguf Q4_K_M 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dedde2-05cc-4711-a345-192271abb421",
   "metadata": {},
   "source": [
    "### Done! Now Download It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcdfd44-d65f-4649-9c82-e293203912ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la ./models/$MODEL_NAME/$MODEL_NAME-Q4_K_M.gguf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
