{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc094eb-85ab-4a88-a9c2-19a3379ad73d",
   "metadata": {},
   "source": [
    "## Evangelista â€“ Hugging Models Quantization - GUFF/GGML  (see also AWQ, GPTQ, SqueezeLLM)\n",
    "- GGUF is a new lib/format that replaces GGML (deprecated). Open source from llama.cpp team, more extensible and user friendly \n",
    "  - Quant Comparisons: https://deci.ai/blog/ggml-vs-gguf-comparing-formats-amp-top-5-methods-for-running-gguf\n",
    "  - Pre-Quantized Models: https://huggingface.co/TheBloke/CodeLlama-34B-GGUF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777b8d96-e127-4448-9657-12caab2cc6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional, Show Machine/Pod Info\n",
    "!uname -a\n",
    "!python --version && echo\n",
    "!pip list | grep torch && echo\n",
    "!lscpu | head -n 8 && echo\n",
    "!nvidia-smi | grep -E 'NVIDIA|MiB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d18133-0a76-4918-88f4-19e40f05d09d",
   "metadata": {},
   "source": [
    "### Clone and Build llama.cpp\n",
    "llama.cpp provides the tools to convert models to gguf and quantize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcbe656-2c83-41e9-bda2-13c08dc01c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt update -qq -y\n",
    "!apt install build-essential cmake -y >/dev/null\n",
    "\n",
    "# Clone llama.cpp\n",
    "!if [ ! -d \"llama.cpp\" ]; then git clone https://github.com/ggerganov/llama.cpp.git; fi\n",
    "%cd llama.cpp\n",
    "\n",
    "# Build llama.cpp\n",
    "%env PIP_ROOT_USER_ACTION=ignore\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q -r requirements.txt\n",
    "!make quantize\n",
    "%cd ..\n",
    "\n",
    "%reset -f\n",
    "print('Done!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb9bef-706f-424c-8d08-84dc3ab3bf9e",
   "metadata": {},
   "source": [
    "### Log into HuggingFace - Needed To Upload Quantized Model  OR  Input Model Is Gated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7299dc5d-5646-425f-b01c-930df48419ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use env variable token if defined, don't restart sessions\n",
    "import huggingface_hub, os\n",
    "huggingface_hub.login(token=os.getenv('HF_ACCESS_TOKEN'), new_session=False, add_to_git_credential=False)\n",
    "\n",
    "# Optionally, Force re-login\n",
    "#huggingface_hub.login(None, new_session=True, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2083c4-8141-45d5-8ec7-54ceaa8ece1c",
   "metadata": {},
   "source": [
    "### Download and Locally Save The Desired Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83673239-d042-46c9-bc29-42dfbe047826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR MODEL URI BELOW\n",
    "# --------------------------------------------------------------------------------\n",
    "%env HF_MODEL_URI = meta-llama/Llama-2-7b-chat-hf\n",
    "\n",
    "import os, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Default CUDA and float16\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_default_device('cuda')\n",
    "torch.set_default_dtype(torch.float16)\n",
    "\n",
    "HF_MODEL_URI = os.environ.get('HF_MODEL_URI')\n",
    "MODEL_NAME = os.path.basename(HF_MODEL_URI)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    HF_MODEL_URI,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_MODEL_URI,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print('Saving model...')\n",
    "tokenizer.save_pretrained(MODEL_NAME)\n",
    "model.save_pretrained(MODEL_NAME)\n",
    "print('Done!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b6092-b45b-4082-b7cc-67780ba5dd89",
   "metadata": {},
   "source": [
    "### Convert Model to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5baef-a406-430f-9d95-6bb990dca74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, there's NO quiet mode\n",
    "import subprocess\n",
    "subprocess.run(['python', 'llama.cpp/convert.py', MODEL_NAME, '--outfile', f'{MODEL_NAME}.gguf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73700bd6-8d51-4faa-8c33-cf3b2d215567",
   "metadata": {},
   "source": [
    "### Quantize to Q4_K_M\n",
    "Quantizations Reference: https://github.com/ggerganov/llama.cpp/pull/1684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58ffb6-2919-4b72-a7aa-5feb5a6684d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_THREADS=16\n",
    "QUANTIZATION_MODE='Q4_K_M'\n",
    "QUANTIZED_MODEL_NAME=f'{MODEL_NAME}-GGUF-{QUANTIZATION_MODE}'\n",
    "QUANTIZED_MODEL_URI=f'{QUANTIZED_MODEL_NAME}.gguf'\n",
    "\n",
    "# Note, there's NO quiet mode\n",
    "subprocess.run(['llama.cpp/quantize', f'{MODEL_NAME}.gguf', QUANTIZED_MODEL_URI, QUANTIZATION_MODE, f'{NUM_THREADS}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dedde2-05cc-4711-a345-192271abb421",
   "metadata": {},
   "source": [
    "### Create HuggingFace Repo & Upload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e35275-1d74-41c4-a550-0e4d01645d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo, HfApi\n",
    "\n",
    "# ENTER YOUR HUGGINGFACE USER ID BELOW\n",
    "# --------------------------------------------------------------------------------\n",
    "HF_USER_ID='bevangelista'\n",
    "REPO_ID=f'{HF_USER_ID}/{QUANTIZED_MODEL_NAME}'\n",
    "\n",
    "# Create Repo -- NOTE: Make sure your token has WRITE permission\n",
    "try:\n",
    "    create_repo(REPO_ID, repo_type='model', private=False)\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "\n",
    "# Upload all files\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    repo_id=REPO_ID,\n",
    "    path_or_fileobj=QUANTIZED_MODEL_URI,\n",
    "    path_in_repo=QUANTIZED_MODEL_URI,\n",
    "    commit_message='Upload quantized models'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2653ebb3-dc9e-4de9-9bf8-ee36cd9c1642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
