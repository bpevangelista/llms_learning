{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc094eb-85ab-4a88-a9c2-19a3379ad73d",
   "metadata": {},
   "source": [
    "## Evangelista â€“ Hugging Models Quantization - GUFF/GGML  (see also AWQ, SqueezeLLM)\n",
    "- GGUF. New library that replaces GGML (deprecated). Open source, more extensible and user friendly.\n",
    "  - Quant Comparisons: https://deci.ai/blog/ggml-vs-gguf-comparing-formats-amp-top-5-methods-for-running-gguf\n",
    "  - Pre-Quantized Models: https://huggingface.co/TheBloke/CodeLlama-34B-GGUF\n",
    "\n",
    "### Clone and Build llama.cpp\n",
    "llama.cpp provides the tools to convert model to gguf and quantize it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcbe656-2c83-41e9-bda2-13c08dc01c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt update -y\n",
    "!apt install build-essential cmake -y >/dev/null\n",
    "\n",
    "# Clone llama.cpp\n",
    "!if [ ! -d \"llama.cpp\" ]; then git clone https://github.com/ggerganov/llama.cpp.git; fi\n",
    "%cd llama.cpp\n",
    "\n",
    "# Build llama.cpp\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q -r requirements.txt\n",
    "!make quantize\n",
    "%cd ..\n",
    "\n",
    "%reset -f\n",
    "print('Done!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb9bef-706f-424c-8d08-84dc3ab3bf9e",
   "metadata": {},
   "source": [
    "### Log into HuggingFace - Needed To Upload Your Model OR If Input Model Is Gated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7299dc5d-5646-425f-b01c-930df48419ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE You ONLY need to login if your model is gated\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2083c4-8141-45d5-8ec7-54ceaa8ece1c",
   "metadata": {},
   "source": [
    "### Download and Locally Save The Desired Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83673239-d042-46c9-bc29-42dfbe047826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR MODEL URI BELOW\n",
    "# --------------------------------------------------------------------------------\n",
    "%env HF_MODEL_URI = meta-llama/Llama-2-7b-chat-hf\n",
    "\n",
    "import torch, os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Default CUDA and float16\n",
    "torch.set_default_device('cuda')\n",
    "torch.set_default_dtype(torch.float16)\n",
    "\n",
    "HF_MODEL_URI = os.environ.get('HF_MODEL_URI')\n",
    "MODEL_NAME = os.path.basename(HF_MODEL_URI)\n",
    "GGUF_MODEL_URI = f'{MODEL_NAME}/{MODEL_NAME}.gguf'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    HF_MODEL_URI,\n",
    "    pad_token='<pad>',\n",
    "    trust_remote_code=True,\n",
    "    token=os.getenv('HF_ACCESS_TOKEN') # optionally, set env var as token for repo access\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_MODEL_URI,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    token=os.getenv('HF_ACCESS_TOKEN') # optionally, set env var as token for repo access\n",
    ")\n",
    "\n",
    "print('Saving model...')\n",
    "tokenizer.save_pretrained(MODEL_NAME)\n",
    "model.save_pretrained(MODEL_NAME)\n",
    "print('Done!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b6092-b45b-4082-b7cc-67780ba5dd89",
   "metadata": {},
   "source": [
    "### Convert Model to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5baef-a406-430f-9d95-6bb990dca74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python ./convert.py ./models/$MODEL_NAME --outfile ./models/$MODEL_NAME/$MODEL_NAME.gguf\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(['llama.cpp/convert', MODEL_NAME, '--outfile', GGUF_MODEL_URI])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73700bd6-8d51-4faa-8c33-cf3b2d215567",
   "metadata": {},
   "source": [
    "### Quantize to Q4_K_M\n",
    "Quantizations Reference: https://github.com/ggerganov/llama.cpp/pull/1684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58ffb6-2919-4b72-a7aa-5feb5a6684d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!./quantize ./models/$MODEL_NAME/$MODEL_NAME.gguf ./models/$MODEL_NAME/$MODEL_NAME-Q4_K_M.gguf Q4_K_M 16\n",
    "\n",
    "NUM_THREADS=16\n",
    "QUANTIZATION_MODE='Q4_K_M'\n",
    "GGUF_QUANTIZED_MODEL_URI=f'{MODEL_NAME}/{QUANTIZED_MODEL_NAME}.gguf'\n",
    "\n",
    "subprocess.run(['llama.cpp/quantize', GGUF_MODEL_URI, '--outfile', GGUF_QUANTIZED_MODEL_URI, QUANTIZATION_MODE, f'{NUM_THREADS}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dedde2-05cc-4711-a345-192271abb421",
   "metadata": {},
   "source": [
    "### Create HuggingFace Repo & Upload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e35275-1d74-41c4-a550-0e4d01645d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo, HfApi\n",
    "\n",
    "# YOUR HUGGINGFACE USER ID BELOW\n",
    "# --------------------------------------------------------------------------------\n",
    "HF_USER_ID='soij'\n",
    "REPO_ID=f'{HF_USER_ID}/{MODEL_NAME}-{QUANTIZATION_MODE}'\n",
    "\n",
    "# Create Repo -- NOTE: Make sure your token has WRITE permission\n",
    "try:\n",
    "    create_repo(REPO_ID, repo_type='model', private=False)\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "\n",
    "# Upload all files\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    repo_id=REPO_ID,\n",
    "    path_or_fileobj=GGUF_QUANTIZED_MODEL_URI,\n",
    "    path_in_repo='/',\n",
    "    commit_message='Upload quantized models'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
